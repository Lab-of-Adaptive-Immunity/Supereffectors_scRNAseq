import sys, os, glob
import gzip, time
import multiprocessing
# @author: Juraj Michalik
# @date: 02/01/2021
# this script splits Fastq files by lists of hashtags that were computed 
# by script 'Supereffectors_PART1_1_generating_sample_lists.Rmd'. 

# REQUIREMENT:
# - specifying paths where fastq files are at the bottom
# - copying hashtag lists with barcodes in the same dir as this script

# LICENSE: GNU GPL v3.
# All scripts are distributed to ease the reproduction of the analysis
# from the above paper, but WITHOUT ANY WARRANTY; without even the 
# implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. 
# See the GNU General Public License for more details.

def list2arr(list_file):
	list_h = open(list_file,'r')
	barcodes = list_h.readlines()
	list_h.close()
	
	barcode_array = [x.strip().strip('"') for x in barcodes]
	return barcode_array
	

def barcode_present(list_barcodes, sequence):
	for barcode in list_barcodes:
		if len(barcode)>0 and barcode in sequence[0:19]:
			return True
			
	return False
	
def write_read(fhandler, header, sequence, sepa, qual):
	fhandler.write(header+sequence+sepa+qual)
	
def	process_lane(sample_id, fasta_path_input, fasta_path_output, hash_names, hash_lists, sample_suffix, n_lane, n_segment):

	lane_num = '%03d'%n_lane
	segment_num = '%03d'%n_segment
	sample_pref = "_".join(sample_suffix.split('_')[:-1])
	print(sample_suffix, sample_pref, n_lane, n_segment, glob.glob(fasta_path_input + '*' + sample_suffix + '*' + '/*' + sample_pref + '*'+ lane_num + '_R1*' + segment_num + '*'))
	reads1 = glob.glob(fasta_path_input + '*' + sample_suffix + '*' + '/*' + sample_pref + '*'+ lane_num + '_R1*' + segment_num + '*')[0] 
	reads2 = glob.glob(fasta_path_input + '*' + sample_suffix + '*' + '/*' + sample_pref + '*'+ lane_num + '_R2*' + segment_num + '*')[0] 
	
	reads1_fh = gzip.open(reads1,'rt')
	reads2_fh = gzip.open(reads2,'rt')
	header1 = 'start'
	
	suffix_r1 = reads1.split('/')[-1]
	suffix_r2 = reads2.split('/')[-1]
	
	# open files where we will write
	reads1_hash = []
	reads2_hash = []
	
	for hash_name in hash_names:
		complete_path = fasta_path_output + '/' + sample_id + '/' + sample_suffix + '/' + hash_name + '_' + sample_suffix
		reads1_hash.append(gzip.open(complete_path + '/' + suffix_r1,'wt'))
		reads2_hash.append(gzip.open(complete_path + '/' + suffix_r2,'wt'))
	
	while(header1):
		header1 = reads1_fh.readline()
		sequence1 = reads1_fh.readline()
		sepa1 = reads1_fh.readline()
		qual1 = reads1_fh.readline()
		
		header2 = reads2_fh.readline()
		sequence2 = reads2_fh.readline()
		sepa2 = reads2_fh.readline()
		qual2 = reads2_fh.readline()
		
		for i,hash_list in enumerate(hash_lists) :
			if barcode_present(hash_list, sequence1):
				write_read(reads1_hash[i], header1, sequence1, sepa1, qual1)
				write_read(reads2_hash[i], header2, sequence2, sepa2, qual2)
				break
	
	reads1_fh.close()
	reads2_fh.close()
	
	for i in range(len(hash_names)):
		reads1_hash[i].close()
		reads2_hash[i].close()

def split_samples(sample_ids, hash_names, hash_files, fasta_path_input, fasta_path_output):
	for i,sample_id in enumerate(sample_ids):
		# transform hashes to arrays
		hash_list = []
		for j in hash_files[i]:
			hash_list.append(list2arr(j))
		
		dirs = glob.glob(fasta_path_input + '*' + sample_id + '*') # list all dirs that have sample name within it
		for directory in dirs:
			sample_suffix = directory.split('/')[-1]
			files = glob.glob(directory + '/*')
			nlanes = list(set([int(x.split('.')[0].split('_')[-3][1:]) for x in files]))
			print(nlanes)
			for k in nlanes:
				files = glob.glob(directory +'/*L%03d*'%k)
				nsegments = list(set([int(x.split('.')[0].split('_')[-1]) for x in files]))
				print(nsegments)
				for l in nsegments:
					#process_lane(fasta_path_input, fasta_path_output, hash_names, hash_list, sample_suffix, k)
					multiprocessing.Process(target=process_lane, args=(sample_id, fasta_path_input, fasta_path_output, hash_names[i], hash_list, sample_suffix, k, l)).start()
		
# MAIN
# [!] You need to adjust some paths
# [!] If you downloaded files from provided link, this is not needed as the files are already demultiplexed.
if __name__ == '__main__':	
	
	# config		
	# [!] you need to specify these input and output files
	fasta_path_input = '/path/with/fastqs/'
	fasta_path_output = '/path/to/output/files'
	
	# names of all files that classify cells
	# [!] these files are generated by 'Supereffectors_PART1_1_generating_sample_lists.Rmd'
	# [!] these files have to be in same directory as this script!
	hash_files1 = ['exp07_W03_Mouse_Hashtag_1.csv', 'exp07_W03_Mouse_Hashtag_2.csv', 'exp07_W03_Mouse_Hashtag_3.csv', 'exp07_W03_Mouse_Hashtag_4.csv', 'exp07_W03_Mouse_Hashtag_5.csv', 'exp07_W03_Mouse_Hashtag_6.csv', 'exp07_W03_Mouse_Hashtag_7.csv', 'exp07_W03_Mouse_Hashtag_8.csv', 'exp07_W03_Mouse_Hashtag_9.csv', 'exp07_W03_Mouse_Hashtag_10.csv']	
	hash_files = [hash_files1]
	
	sample_ids = ['exp07-W3']
	
	# names of hash files: WARNING: these should be in same order as lists
	hash_names1 = ['Mouse_Hashtag_1', 'Mouse_Hashtag_2','Mouse_Hashtag_3', 'Mouse_Hashtag_4', 'Mouse_Hashtag_5', 'Mouse_Hashtag_6', 'Mouse_Hashtag_7', 'Mouse_Hashtag_8', 'Mouse_Hashtag_9', 'Mouse_Hashtag_10']	
	hash_names = [hash_names1]
	
	# find dirs with various fastq
	samples = glob.glob(fasta_path_input+'*')
	suffixes = [x.split('/')[-1] for x in samples]

	print(suffixes)
	print(samples)
	
	# build file tree if not created yet
	for c, i in enumerate(sample_ids):
		for j in suffixes:
			for k in hash_names[c]:
				if not os.path.exists(fasta_path_output+'/'+i+'/'+j+'/'+k+'_'+j):
				  if(i in j):
				    os.makedirs(fasta_path_output+'/'+i+'/'+j+'/'+k+'_'+j)
				
	split_samples(sample_ids, hash_names, hash_files, fasta_path_input, fasta_path_output)
